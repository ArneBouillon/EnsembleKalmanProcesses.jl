# Cloudy Example

### What Is This Example About?

This example is based on Cloudy, a microphysics model that simulates how cloud droplets collide and coalesce into larger drops. Collision-coalescence is a crucial process for the formation of rain. 

Cloudy takes two inputs: The initial mass distribution of the cloud droplets, and a so-called collision-coalescence kernel, which specifies the probability that two droplets of masses ``x`` and ``y`` collide and coalesce in a given time interval. 
Cloudy simulates the evolution of the initial cloud droplet mass distribution over time, as more and more droplets combine into bigger drops. It does this not by simulating each individual droplet, but by using a "bulk approach", which only keeps track of some number of moments of the cloud droplet mass distribution.

This example shows how Ensemble Kalman methods can be used to learn the parameters of the initial cloud droplet mass distribution from observations of the moments of that mass distribution at a later time. The collision-coalescence kernel is assumed to be known, but one could also learn the parameters of the kernel instead of the parameters of the droplet distribution (or both).

Cloudy is used here in a "perfect model" (aka "known truth") setting, which means that the "observations" are generated by Cloudy itself, by running it with the true parameter values. In more realistic applications, the observations will come from some external measurement system.

For more information on Cloudy, see [here](https://github.com/CliMA/Cloudy.jl.git).


### Prerequisites

In order to run this example, you need to install `Cloudy.jl` (the "#master"
lets you install the current master branch):
```
pkg > add Cloudy#master
```

### Structure

The file `DynamicalModel.jl` provides the functionality to run Cloudy, i.e., to map the parameters of the initial cloud droplet mass distribution to the (zeroth-, first-, and second-order) moments of the distribution at a specified end time. In this example, there is no post-processing required to produce the observations from the model output (i.e., the observation map ``\mathcal{H}`` mentioned in the [docs](https://clima.github.io/EnsembleKalmanProcesses.jl/dev/ensemble_kalman_inversion/) is the identity map).

The file `Cloudy_example_eki.jl` sets up the inverse problem and solves it using [ensemble Kalman inversion](https://clima.github.io/EnsembleKalmanProcesses.jl/dev/ensemble_kalman_inversion/), and the file `Cloudy_example_uki.jl` does the same using unscented Kalman inversion.


### Inverse Problem

The assumed functional form of the cloud droplet mass distribution ``f(x, t)`` (where ``x`` is the droplet mass and ``t`` is time) is a ``Gamma(k_t, \theta_t)`` distribution, scaled by a factor ``N_{0,t}`` which denotes the droplet number concentration:
```math
f(x, t) = \frac{N_{0,t}}{\Gamma(k_t)\theta_t^k} x^{k_t-1} \exp{(-x/\theta_t)}
```
The parameter vector ``\phi_t= [N_{0,t}, k_t, \theta_t]`` changes over time (as indicated by the subscript ``t``), as the shape of the distribution evolves. In fact, there is a priori no reason to assume that the distribution would retain its Gamma shape over time, but this is a common assumption that is made in order to solve the closure problem (generally, the prognostic moments depend on higher-order
moments which can only be calculated when an assumption is made on the type of distribution giving rise to those moments).

The goal is to learn the parameters at time ``t = 0``, ``\phi_0 = [N_{0,0}, k_0, \theta_0]``, from observations ``y`` of the zeroth-, first-, and second-order moments of the distribution at time ``t_{end} > 0``. All three parameters have to be strictly positive, so we put lower bounds on them and use a log transformation ``\mathcal{T}`` to map the parameter ``\phi`` to a parameter ``\theta`` (i.e., ``\mathcal{T}(\phi) = \theta``), which lives in an unconstrained space. The ensemble Kalman algorithm is then performed in this unconstrained "``\theta``-space" (note: don't confuse the parameter vector ``\theta`` with the ``\theta_t`` parameter of the Gamma distribution).

The data ``y`` and parameter vector ``\theta`` are assumed to be related according to:
```math
    y = \mathcal{G}(\theta) + \eta,
```
where ``\mathcal{G}:  \mathbb{R}^3 \rightarrow \mathbb{R}^3`` denotes the  forward map, which is composed of the dynamical model ``\Psi`` (here: Cloudy)  and the inverse transformation ``\mathcal{T}^{-1}``: ``\mathcal{G} = \Psi  \circ \mathcal{T}^{-1}``. 

``y \in \mathbb{R}^3`` is the vector of observations,  and ``\eta`` is the observational noise, which is assumed to be drawn from a  3-dimensional  Gaussian with distribution ``\mathcal{N}(0, \Gamma_y)``.
 
In a perfect model setting, the "observational noise" represents the internal model variability. Since Cloudy is a purely deterministic model, there is no straightforward way of coming up with a covariance ``\Gamma_y`` for this internal noise. We decide to use a diagonal covariance, with entries (variances) largely proportional to ``\mathcal{G}(\theta_{true})``, where ``\theta_{true}`` is the true distribution parameter vector (in unconstrained space).


### Running the Example

Once Cloudy is installed, the examples can be run from the julia REPL:
```julia
# Solve inverse problem using ensemble Kalman inversion
include("Cloudy_example_eki.jl")
```
or
```julia
# Solve inverse problem using unscented Kalman inversion
include("Cloudy_example_uki.jl")
```

### Solution and Output

* `Cloudy_example_eki.jl`: The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration, which is printed to standard output. An output directory is created, where two files are stored: `parameter_storage_eki.jld2` and `data_storage_eki.jld2`, which contain all parameters and model output from the ensemble Kalman iterations, respectively (both as `DataStorage.DataContainer` objects). In addition, an animation is produced that shows the evolution of the ensemble of particles over subsequent iterations of the optimization.

* `Cloudy_example_uki.jl`: In addition to a point estimate of the optimal parameter (which is again given by the ensemble mean of the last iteration and printed to standard output), Unscented Kalman inversion also provides a covariance approximation of the posterior distribution. Together, the mean and covariance allow for the reconstruction of a Gaussian approximation of the posterior distribution. The evolution of this Gaussian approximation over subsequent iterations is shown as an animation. All parameters as well as the model output from the unscented Kalman inversion are stored in an output directory, as `parameter_storage_uki.jld2` and `data_storage_uki.jld2`.


### Playing Around
If you want to play around with the Cloudy examples, you can e.g. change the type or the parameters of the initial cloud droplet mass distribution (see `Cloudy.ParticleDistributions` for the available distributions), by modifying these lines:
```julia
ϕ_true = [N0_true, θ_true, k_true]
dist_true = ParticleDistributions.GammaPrimitiveParticleDistribution(ϕ_true...)
```
(Don't forget to also change `dist_type` accordingly).

You can also experiment with different noise covariances (`Γy`), priors, vary the number of iterations (`N_iter`) or ensemble particles (`N_ens`), etc.
